The Atomistic Machine-learning Package (AMP) 
\parencite[Khorshidi, Alizera and Peterson, Andrew A.]{khorshidi2016amp}
is a software package
written in Python with the intent of bringing machine learning
to electronic structure calculations. The software is intended
to interface with ASE and the OpenKIM API\footnote{\url{
    https://openkim.org/}}
for usage in LAMMPS.
The interface to AMP is written purely in Python
while computationally intensive tasks are outsourced to Fortran
modules and supports neural networks through both Python code
and through a Tensorflow backend.
The Python interface makes AMP flexible and easily
extended, and this makes the package ideal for prototyping
and testing both newer and more established machine learning
methods.
\par
A suggested workflow is as follows\footnote{\url
{https://bitbucket.org/andrewpeterson/amp/issues/79/parallelize-fingerprint-derivatives}}:

\begin{itemize}
    \item Use AMP for training, testing and validation
        of novel descriptors and systems
    \item Use the AMP calculator for smaller scale
        simulation in the ASE environment
    \item Export the network compliant with the OpenKIM
        API for usage in more mature, large-scale
        electronic structure calculation software such as LAMMPS
\end{itemize}

Unfortunately, the software is not currently fully compliant
with the latest version of the OpenKIM API,
which introduces artifacts in the simulation involving periodic images.
However, the authors have recently received a large grant
from the U.S. Department of Energy\footnote{\url{
https://www.brown.edu/news/2018-09-20/simulations}}, 
which should facilitate further development.

\subsection{Theory}
AMP is intended to interface extensively with ASE,
and the primary interface to AMP is the AMP calculator.
The AMP calculator is an ASE compliant calculator
that accepts cartesian coordinates and outputs energies
and forces, just as a classical molecular dynamics calculator.
The primary difference between the AMP calculator
and ASE calculators such as the Lennard-Jones calculator
is the \textit{train} method, which accepts a set of \textit{images}, i.e.
a set of snapshots of atomic configurations labeled
with the potential energy and forces.
The calculator is fitted to the images and can subsequently be
used to predict atomic energies and forces from never before seen
atomic configurations.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{scheme.png}
\caption{Schematics of how AMP works in atom-centered mode.
    Reprinted from \parencite[AMP paper]{khorshidi2016amp}.}
\label{fig:scheme}
\end{figure}

AMP provides interfaces for both atom-centered descriptors, as we
discussed in chapter 6, and image-centered descriptors,
which are formed from the complete set of cartesian coordinates.
Figure \ref{fig:scheme} shows the schematics of how the
potential energy and forces is computed in atom-centered mode.
A set of feature vectors are formed from an image, which are fed
through a neural network to produce the potential energy.
The neural network including backpropagation is written by the
authors, though there exists a Tensorflow (0.12) interface
only compatibable with Python 2.
The inputs can be backpropagated through the network
to produce both weight updates and derivatives with respect to
the inputs.

\subsection{Installation}
AMP requires an installation of

\begin{itemize}
    \item Python 2.7, 3.4-3.6, 3.6 is recommended
    \item Numpy 1.9 or newer
    \item Scipy 0.14 or newer
    \item ASE
\end{itemize}

Python installations can be easily obtained through the Anaconda 
or Miniconda packages\footnote{\url{https://anaconda.org/}},
or follow the instructions on the Python website\footnote{
\url{https://www.python.org/}}.
ASE installation is described in the chapter on ASE.
Once you have the prerequisites AMP can be installed using pip:

\begin{lstlisting}[language=bash]
pip install amp-atomistics
\end{lstlisting}

\subsection{Training example}
In the previous chapter we showed how to run a molecular dynamics
simulation using ASE. Here we will show how an AMP calculator
can be fitted to molecular dynamics data with only minor
modifications to the code.
First we import the prerequisites and define the system:

\begin{minted}{python}
import ase.io
from ase.lattice.cubic import FaceCenteredCubic
from ase import units
from ase.md.velocitydistribution import MaxwellBoltzmannDistribution
from ase.md.verlet import VelocityVerlet

from amp import AMP
from amp.descriptor.gaussian import Gaussian
from amp.model.neuralnetwork import NeuralNetwork
from amp.model import LossFunction

symbol = "Ar"
size = (3, 3, 3)
atoms = FaceCenteredCubic(symbol=symbol, size=size, pbc=True)
MaxwellBoltzmannDistribution(atoms, 300 * units.kB)
\end{minted}

The class AMP is the primary object of the AMP package,
and is implemented through the ASE interface.
We will use a neural network machine learning model
and Gaussian descriptors, which are an implementation of the
Behler-Parrinello method. To implement force training we require
access to the LossFunction class.
We will also be using the ASE Input/Output (ase.io) module to generate an
ASE \textit{Trajectory}, which is an object
storing the time-evolution of a simulation and usually interpreted
as a time series of atoms.

\begin{minted}[firstnumber=last]{python}
traj = ase.io.Trajectory("training.traj", "w")
calc = LennardJones(sigma=3.405, epsilon=1.0318e-2)
atoms.set_calculator(calc)
atoms.get_potential_energy()
atoms.get_forces()
traj.write(atoms)
dyn = VelocityVerlet(atoms, 2 * units.fs)
\end{minted}

In order to write the potential energy and forces to file
they must first be calculated using the ASE Atoms methods,
which require a calculator and otherwise raise an error.
We can then evolve the system forward in time
and save the atomic configuration every 10 time steps:

\begin{minted}[firstnumber=last]{python}
for i in range(100):
    dyn.run(10)
    atoms.get_potential_energy()
    atoms.get_forces()
    traj.write(atoms)
\end{minted}

After the data has been generated we can train the AMP calculator
using the Lennard-Jones calculations as input. 
We use a neural network model with three hidden layers,
each containing 10 neurons.
The training procedure
will terminate once the energy and force root mean squared errors (RMSE)
have reached values of $10^{-2}$ or less:

\begin{minted}[firstnumber=last]{python}
calc = Amp(descriptor=Gaussian(),
           model=NeuralNetwork(hiddenlayers=(10, 10, 10)))
calc.model.lossfunction = LossFunction(
           convergence={"energy_rmse": 1E-2,
                        "force_rmse": 1E-2})
calc.train(images="training.traj")
\end{minted}

Once the calculator has been trained, the parameters
are stored in a file called "amp.amp",
and can be loaded using the command:

\begin{minted}{python}
calc = Amp.load("amp.amp")
\end{minted}

From there the calculator can be used as any other ASE calculator,
in electronic structure calculations on Atoms objects.

\subsection{Descriptors and models}
Currently AMP provides support for three different descriptors.
Gaussian descriptors implement the Behler-Parrinello method
of radial and angular symmetry functions. For this descriptor
the derivatives are also available, which means it can be used
for computing dynamics. Some physical intuition and chemical knowledge
is necessary to choose various parameters for the symmetry functions -
unless you wish to do it by trial and error - but AMP provides
defaults for multiple chemical species.
The Zernike and Bispectrum descriptors are also implemented,
but without derivatives, which means the trained calculator
cannot be used to for example perform molecular dynamics simulation.
Currently the neural network and kernel ridge regression\footnote{
    See for example: \url{
        https://scikit-learn.org/stable/modules/kernel_ridge.html}}
models are the only models implemented.
However, the modular nature of AMP should enable more additions
in time, such as derivatives for the Zernike and Bispectrum descriptors
or other regression models which have derivatives available.
