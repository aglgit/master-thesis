In this thesis we have trained neural networks to reproduce
molecular dynamics potentials using the Behler-Parrinello method.
These potentials are high-dimensional functions with many parameters
to be determined, and are often developed through intimate knowledge
of the physical and chemical properties of the systems they are designed for.
In addition there are certain symmetries which must be respected when
developing potentials, in particular translational, rotational and permutational
symmetries, as well as conserving energy over time (in the NVE ensemble).
Traditional ab-initio methods suffer from poor scaling as the size of the systems
increases, while classical potentials derived from ab-initio calculations
allows us to simulate realistic scales of up to millions of atoms
depending on the computer resources and the complexity of the atoms in the system.
Since these neural networks when trained offer linear scaling,
they could serve both as supplements to ab-initio methods
or be deployed for calculations using classical molecular dynamics.
For example, one could save many CPU cycles by producing and training
on a small trajectory calculated from DFT, and then use the neural networks
to produce the remainder of the data, provided the neural networks
are reasonably accurate.
While our neural network potential scales linearly with system size,
it has a rather large pre-factor dependent on the symmetry function set,
cutoff radius and the average number of neighbors in the system.
We believe this could be improved by moving more calculations to lower
level compiled languages, such as calculations of neighbor lists,
fingerprints and fingerprint derivatives for every step.
Although we have only ran the neural networks on a single core,
the potential could be parallelized over the atoms
using algorithms such as LAMMPS neighbor lists without
too much overhead.
\par
Unfortunately we were not able to achieve energy conservation
with our neural networks. 
This a central feature of any neural network potential,
and without energy conservation, we cannot sample properties in equilibrium,
as for example the radial distribution function or the diffusion constant
is increasing over time and ill defined.
Generally we find an increase in kinetic energy over time, corresponding
to an increase in potential energy as the atoms move apart,
and an increase in translational momentum.
This indicates holes in the training data from sampling in equilibrium,
as the network seems to perform poorly on unseen configurations with larger
forces, and produce large force residuals.
The results are notably different for the EMT and Stillinger-Weber potential,
this is caused by among other things the symmetry function sets and the
average number of neigbors in the system.
The Stillinger-Weber explicitly includes three-body interactions,
while the EMT potential is a function of interatomic distances, and our
results suggest that the balance between the number of radial and symmetry functions
should be decided by more careful analysis of their relative importance.
\par
On the test trajectory we achieved reasonable values of the energy and force
root mean squared errors, approximately 0.1 eV for the energy and 0.05-0.1 eV/Ã…
for the force. These are mostly consistent with the results others have achieved
such as \cite{stende2017constructing, treider2017speeding, khorshidi2016amp,
    PhysRevLett.120.143001},
though the energy is often on the order of 1-10 meV, which is lower than
what we have achieved. This is partly due to the fact that we have emphasized
training with forces, which comes at the expense of the energy fit.
Since the training Root Mean Squared Errors (RMSEs) 
is substantially lower than the test RMSE this may
indicate overfitting, though we would not expect this to be a substantial problem
with such a small network with applied regularization.
We note that this has not been tested extensively,
and more testing could produce different results.

\subsection{Prospects and future work}
In this thesis we barely scratched the surface of combining machine learning
methods with molecular dynamics. There are therefore many, many paths
to be explored for future theses or even articles to be published,
which may be achieved working through ASE and/or AMP and making modifications
or writing your own code from scratch. We will list some of the prospects
considered while writing this thesis in semi-ordered order of importance,
though there are likely many which have not been considered.

\begin{itemize}
    \item Numerical optimization:
        In order to speed up the training and deployment of neural
        networks the Atomistic Machine-learning Package (AMP)
        authors created efficient Fortran implementations
        of the Behler-Parrinello symmetry functions.
        This is an improvement over pure Python code, but not enough to
        be satisfied.
        Many parts of the fingerprinting, including neighborlists,
        dictionary data structures, IF tests and so on are currently
        implemented in Python code. This means we are limited in terms
        of CPU time and memory in how fast we can alter hyperparameters
        and train neural networks on new datasets, and how quickly
        the neural networks can be deployed and evaluated in molecular dynamics.
        First and foremost we would suggest moving the calculation of
        the neighborlists, fingerprints and fingerprint derivatives entirely
        to Fortran or some other efficient compiled language.
        This would reduce the speed of evaluations which are currently performed
        in Python, and reduce communication overhead between the Python APIs
        and the Fortran compiled functions.
        For more information see \href{
            https://listserv.brown.edu/cgi-bin/wa?A2=AMP-USERS;d7c6c98c.1904}{
            this post} on the AMP mailing list.
        Once we have our input data and labels, training can be performed
        efficiently using software packages such as Tensorflow or Pytorch,
        and training moved to GPUs with little effort.
    \item Parallelization and LAMMPS:
        As we have discussed in the chapter on the Atomistic Machine-learning Package,
        AMP is planned to provide support for the OpenKIM API which would provide
        the means to export the neural networks to LAMMPS molecular dynamics
        software for deployment.
        If this is implemented (currently unknown) this could significantly speed up
        the deployment and testing of the neural networks, as LAMMPS 
        is an efficient compiled code,
        with efficient algorithms for parallelizing the force evaluations
        over multiple cores.
    \item Improved sampling method:
        We have observed that sampling from molecular dynamics trajectories
        limits us mostly to systems in equilibrium, which limits the
        range of energies and forces observed in the dataset.
        Neural networks perform unexpectedly when encountering unseen data,
        and the training data therefore restricts the generalization
        properties of the network. Improved sampling algorithms to
        sample a wider range of energies and forces out of equilibrium
        would likely improve the long timescale performance of the neural
        network and make the neural network potential more accurate on
        new configurations.
    \item Neural network implementation:
        Currently AMP provides a neural network implementation written
        by the authors, and a Tensorflow 0.11 module compatible only
        with Python 2.7. We would suggest writing a more modern Tensorflow
        interface, using for example the new Tensorflow 2.0 beta,
        in order to take full advantage of these mature neural network
        implementations. This could among other things improve speed,
        more efficient algorithms for initialization and training
        and a large set of helpful functions for training neural networks
        efficiently.
    \item Determining symmetry function sets:
        We have discussed some methods of determining symmetry functions,
        but not in great detail. Generally you want the symmetry functions
        to cover the radial and angular space, while no two symmetry functions
        should be highly correlated. However, we have observed that
        the same general mix of symmetry functions can produce very
        different results. It would be very beneficial if we had an automated
        approach or highly specified approach to determining the numbers of
        and parameters of the symmetry functions for a wide range of
        potential energy surfaces, which would significantly ease the difficulty
        in training and deploying neural networks using the Behler-Parrinello
        method.
   \item Finding minima:
        The AMP package finds minima in the loss function using an
        interface to the scipy.optimize library of functions.
        In this thesis we have only tested with the BFGS optimizer,
        which is the default in AMP, and the one generally favored
        by the authors and other users. However, finding minima in the cost
        function of neural networks is a complicated affair, which has
        been examined in great detail in the literature.
        First and foremost one could test other optimizer available
        through scipy, such as the basin hopping optimizer.
        If an interface to a more mature neural network software package
        such as Tensorflow is 
        implemented it would be easy to test optimizers such as 
        ADAM, SGD, Adagrad and many more \cite{kingma2014adam}.
    \item New descriptors:
        In this thesis we have restricted our interest to the tried and tested
        Behler-Parrinello symmetry functions as the mapping from
        coordinates to inputs. There have since been many suggestions
        on how to fingerprint atomic systems, such as DPMD, SOAP,
        Zernike and Bispectrum descriptors and so forth
        \cite{PhysRevLett.120.143001, bartok2013representing, khorshidi2016amp},
        some of which are discussed in chapter
        \ref{chap:acd}.
        If the forces can be calculated efficiently and accurately
        these descriptors should be evaluated for use in molecular dynamics.
    \item New machine-learning models:
        In this thesis we have only tested neural networks as the
        machine-learning method for regression, due to the
        explosion of interest and application in recent times.
        Neural networks are favorable due to their ability to scale
        well as the data available increases, but if data is limited
        other machine-learning algorithms may be competitive.
        Since we require the calculation of forces for usage
        in molecular dynamics we require the algorithm to have
        continuous derivatives, and a good example is Kernel Ridge
        Regression, which is currently supported in AMP.
    \item Multiple atom types:
        In this thesis we have limited our attention to single-atom
        systems, though AMP provides support for multiple atoms.
        The Behler-Parrinello method is limited in that a neural network
        has to be trained for every type-type interaction, for example
        an O-H interaction must be treated separately. This means that
        fewer potential energy labels are available for training per neural
        network, and creates a combinatorial problem as the number of interactions
        in the system increases. Suggestions have been made to solve this
        problem such as weighted atom-centered symmetry functions (WACSFs \cite{
            gastegger2018wacsf})
        and these could be implemented and evaluated against
        empirical potentials and neural networks trained using the standard
        approach.
    \item Long-range interactions:
        The Behler-Parrinello approach we have deployed is currently
        limited to short-range interactions within a cutoff sphere,
        and this is not sufficient for long-range interactions such
        as the Coulomb interaction. Using methods such as Ewald summation \cite{
            toukmaji1996ewald},
        the Behler-Parrinello method could accommodate this, and this
        would facilitate training on systems containing for example
        water or biological molecules.
\end{itemize}

