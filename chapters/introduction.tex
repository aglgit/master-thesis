The field of artificial intelligence (AI) has experienced a tremendous
growth in recent years, with a large growth in the number of published
papers, and an influx in funding from both universities and
commercial entities.
The exponential growth of available and high quality
data has brough about a demand for accurate, efficient
and semi-automated algorithms, capable of building complicated
mathematical models and making predictions without human supervision.
Artificial neural networks (ANNs) in particular, partially inspired by simple
models of human cognition, have found success in a large variety
of tasks, largely owing to their ability to scale efficiently
as the size of the dataset grows.
Many trace this recent AI renaissance to
AlexNet\cite{NIPS2012_4824},
which was a convolutional neural network that won the
Imagenet Large Scale Visual Recognition Challenge in 2012\footnote{
    \href{http://www.image-net.org/challenges/LSVRC/}{Imagenet Large Scale
    Visual Recognition Challenge}}.
Artificial intelligence, or machine learning (ML) as it is known within
the literature, can now boast state-of-the-art performance in areas such
as image recognition and analysis, computer vision, speech recognition
and natural language processing\cite{DBLP:journals/corr/abs-1803-01164}.
\par
Data analysis and machine learning has found application
in many subfields of physics, such as observational astronomy,
condensed matter, and subatomic particle physics
\cite{ball2010data}\cite{carrasquilla2017machine}\cite{
    baldi2014searching}.
Quantum chemistry, and electronic structure calculations in particular
is an area which could be well suited for the
adoption of machine learning methods, as large datasets
can be produced on demand and relatively free of noise which
obscures patterns.

\subsection{Electronic structure calculations}
Electronic structure calculations - or numerical solutions
to the many-body Schr\"{o}dinger equation - are methods of modelling
physical systems based on the principles of quantum mechanics.
They involve very few parameters, and the results are found
to be very accurate compared to laboratory measurements.
However, exponential time complexity as the system grows in size
greatly limits their applications,
and we are in general limited to very small and simple systems.
Approximations can be made which reduces the complexity 
to polynomial, but this also limits the accuracy of the method.
The most succesful and widely applied methods
are the Density Functional Theory (DFT) method
and the Hartree-Fock (HF) family of methods.
The Hartree-Fock method's time complexity 
scales nominally as $\mathcal{O}(N^4)$,
where $N$ is the number of particles in the system
(i.e. electrons), owing to the number of two-electron
integrals which must be computed.
However, as the system grows in size many of these spatial integrals
become vanishingly small and can be neglected, at the cost of
introducing a small and adjustable error.
A major weakness of Hartree-Fock methods is the neglect
of electron correlation energies, which can lead to
large deviations from experiment.
The Density Functional Theory method scales in a similar
manner, but often with a larger proportionality term.
Density Functional Theory can treat both exchange and correlation
interactions, but this must be done approximately,
and developing accurate and numerically stable approximations
remains an active area of research \cite{gillan2016perspective}.
\par
Molecular Dynamics (MD) freezes out the degrees of freedom of the
electrons, and treats the atoms as point particles centered on
the nucleus using a mean-field approach.
The dynamics of the particles are treated as Newtonian,
meaning the position and velocity can be integrated using a
symplectic integrator such as Verlet integration
    (see for example \cite{martys1999velocity}).
This treatment allows for systems much larger
than ab-initio methods, but at the cost of neglecting
both nuclear and electronic phenomena.
However, the development of molecular dynamics potentials
involves a large amount of parameters for any realistic system,
and determining their functional form is more of an
art than an exact science.
\par
An application of machine learning which has shown some promise
is the development of molecular dynamics potentials
or Potential Energy Surfaces (PES)\cite{behler2016perspective}.
Using for example
artificial neural networks, the parameters (i.e. weights and biases)
are fully determined from the training data, and the amount
of handcrafting involved can be greatly reduced.
Under mild assumptions imposed on the activation functions in the
hidden layers, ANNs have been proven to be able to approximate
any continuous functions on compact subsets of $\mathbb{R}^N$\cite{
    hornik1989multilayer}.
This offers the potential of developing potential energy surfaces
directly from ab-initio data or from mature and tested
empirical potentials.

\subsection{Atom-centered descriptors}
For electronic structure calculations the potential energy,
forces and other derived properties are determined by
the cartesian coordinates. However, it is not sufficient
to feed neural networks cartesian coordinates labeled with
the energy. From physical theory we know that any molecular dynamics potential
must be invariant w.r.t.\ translation, rotation, and permutation
of like atoms.
In order to apply machine learning methods we require a mapping from
the cartesian coordinates to a 1D feature vector which conserves
all of these properties.
\par
In their article introducing the Atomistic Machine-learning Package (AMP),
the authors \parencite[Khorshidi, Peterson]{khorshidi2016amp}
describe the general process.
The idea is to approximate the potential energy with a regression model:
\begin{equation}
\left\{\bm{R}\right\} \overset{\text{regression}}{\longrightarrow}
    E = E\left( \left\{\bm{R}\right\} \right) ,
\end{equation}
where $\left\{\bm{R}\right\}$ represents the cartesian coordinates of our system.
We will refer to a mapping which satisfies the constraints of 
translation, rotation and permutation invariance as a \textit{descriptor}. 
The descriptor is a multidimensional function $\bm{G}$ that serves as input to
the regression method:
\begin{equation}
    \left\{\bm{R}\right\} \rightarrow \bm{G} \left(\left\{\bm{R}\right\} \right)
\overset{\text{regression}}{\longrightarrow}
E = E \left(\bm{G}\left(\left\{\bm{R}\right\}\right)\right) .
\end{equation}
Once we have a descriptor and a regression model the dynamics
can be readily obtained by taking derivatives. The force on atom $i$
is calculated as:
\begin{equation}
\begin{split}
    \bm{F}_i &= -\nabla_i E \\
    &= -\nabla_i
    E(\bm{G}(\left\{\bm{R}\right\})) \\
    &= -\sum_j \frac{\partial E}
    {\partial G_j} \frac{\partial G_j}{\partial \bm{R}_i} ,
\end{split}
\end{equation}
where we have applied the chain rule to break the gradient
into derivatives with respect to the network inputs
and derivatives of the network inputs with
respect to the coordinates of atom $i$.
The derivatives with respect to the network inputs are obtained
through backpropagation \cite{rumelhart1988learning}, while
the derivatives of the network inputs can be obtained analytically
or numerically.
The potential energy is typically also broken into
atomic contributions:
\begin{equation}
E = \sum_{i=1}^N E_{\text{atom}}\left(\bm{R}_i, \left\{\bm{R}\right\} \right) ,
\end{equation}
where each atomic contribution is determined by the atom's
local environment. This allows us to treat systems with
a varying amount of particles without retraining
the machine learning method each time.

\subsection{Goals}
The goal of this thesis was to investigate machine learning
as a tool for bridging the gap between ab-initio electronic structure
calculations and molecular dynamics. Previous theses at the University
of Oslo's Computational Physics group have investigated the Behler-Parrinello
\cite{behler2007generalized} method of atom-centered symmetry functions,
in particular the theses of
\parencite[Stende, John A,]{stende2017constructing} and
\parencite[Treider, H{\aa}kon Vik{\o}r]{treider2017speeding}.
In these theses they have evaluated the
numerical speed and accuracy of potentials trained on data
produced from the Lennard-Jones
and Stillinger-Weber potentials.
We sought to continue this work, and demonstrate and validate machine-learned
potentials developed using the Behler-Parrinello method, but also apply
the method to data obtained using ab-initio molecular dynamics
and compare these potentials to empirical potentials.
This was initially attemped using a combination of Tensorflow\footnote{
    \url{https://www.tensorflow.org/}} and LAMMPS\footnote{
    \url{https://lammps.sandia.gov/}}.
However, this was later abandoned, as connecting software packages written
in different programming languages proved to be tedious and error-prone,
and the Atomic Simulation Environment (ASE\footnote{
    \url{https://wiki.fysik.dtu.dk/ase/}}) package provided a Python\footnote{
    \url{https://www.python.org/}} 
interface to electronic structure calculations.
Initially we sought to implement and test the Deep Potential Molecular Dynamics
(DPMD) method by \parencite[Zhang et al.]{PhysRevLett.120.143001}, but
it proved difficult to obtain acceptable results for the energy
and force root mean squared errors (RMSEs), and calculating the forces
efficiently proved too difficult to implement. This was therefore abandoned
in favor of the more tried and tested Behler-Parrinello method.
\par
We finally settled on a combination of using the ASE package for producing
molecular dynamics trajectories and the Atomistic Machine-learning Package
(AMP\footnote{\url{https://amp.readthedocs.io/en/latest/}}) 
for training them. 
The AMP package provides an interface
for generating Behler-Parrinello "fingerprints" of atomic environments
and training them using neural networks (and potentially other machine-learning
models). This allowed us to focus on the accuracy, speed and scaling
when training neural networks on empirical potentials and ab-initio
data in molecular dynamics, and how accurately equilibrium properties
can be replicated using trained neural networks.
Unfortunately, we did not have time to generate ab-initio trajectories,
as there are many details which have to be considered when performing
density functional theory calculations, and the calculations are very
time-consuming. Further work in this area could focus on generating
and reproducing trajectories using either Velocity-Verlet dynamics
and ground state DFT calculations or time-dependent density functional
theory molecular dynamics described for example in the 
GPAW calculator documentation
\footnote{\href{https://wiki.fysik.dtu.dk/gpaw/documentation/ehrenfest/ehrenfest_theory
.html}{Ehrenfest theory}}.

\subsection{Contributions}
Ideally we might have liked to make our own implementation
of the Behler-Parrinello method,
and I would have especially liked to build a neural network interface
through the recently released Tensorflow 2.0 (beta). Unfortunately there
was no time for this, and we settled for working through the AMP interface
and making modifications as necessary.
Developing code from scratch could have given insights into the structure
and process of calculating fingerprints and feeding them through neural
networks to produce energies and forces. However, the nature of AMP
is modular and the code is well documented and readable, so 
settling on making modifications to the package can allow one to focus
more on the process of generating and sampling data, training and
deploying the trained neural networks on novel systems and evaluating the results.
Suggestions for future work on the topic
is included in our final conclusion.
In this thesis we have:

\begin{itemize}
    \item Generated and sampled from classical
        molecular dynamics trajectories
    \item Trained and tested neural network potentials
        on data generated from molecular dynamics
    \item Evaluated the ability of neural network potentials
        to reproduce equilibrium properties of classical
        molecular simulations
    \item Written a set of analysis and post-processing scripts
    \item Minor extensions and modifications to the AMP library
        for personal use
\end{itemize}

\subsection{Structure}
This thesis is divided into four parts. First we build from first principles
to connect classical molecular dynamics with the laws of quantum mechanics.
We go through the basics of machine learning
and connect it to molecular dynamics through the concept of atom-centered descriptors.
Then we we discuss the implementation details of using
ASE, AMP and Tensorflow. Third we train neural networks potentials
on empirical potentials and generate new molecular dynamics trajectories
using these potentials.
Fourth we conclude the thesis and discuss future applications and work.
