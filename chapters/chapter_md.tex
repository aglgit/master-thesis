Our starting point for molecular dynamics is
the full Hamiltonian for a set of $N$ electrons and $A$ nuclei:

\begin{equation}
    \begin{split}
        \hat{H} 
        &= -\sum_{i=1}^N \frac{1}{2} \nabla_i^2
        -\sum_{a=1}^A \frac{1}{M_a} \nabla_a^2
        -\sum_{i=1}^N \sum_{a=1}^A \frac{Z_a}{r_{ia}} \\
        &+ \sum_{i=1}^N \sum_{j=i+1}^N \frac{1}{r_{ij}}
        + \sum_{a=1}^A \sum_{b=a+1}^A \frac{Z_a Z_b}{R_{ab}}
    \end{split} .
\end{equation}

We want to find solutions to the time-dependent non-relativistic
Schrodinger equation:

$$ i\hbar \frac{\partial}{\partial t} \Psi = \hat{H} \Psi . $$

\subsection{From quantum mechanics to molecular dynamics}

We will follow the route of Tully described in
[https://core.ac.uk/download/pdf/35009882.pdf?repositoryId=810](here).
The wavefunction is separated in terms of the electronic and nuclear
coordinates with an ansatz

$$ \Psi(\set{\bm{r}_i}, \set{\bm{R}_I}, t)
    \approx \Psi(\set{\bm{r}_i}) \chi(\set{\bm{R}_I})
    \exp \left[ \frac{i}{\hbar} \int_{t_0}^t
    dt^{'} \hat{E}_e(t^{'}) \right] ,
$$

with the electronic and nuclear wavefunctions normalized to unity
at every instance of time. A phase factor is introduced to make
the equations look nice:

$$ \hat{E}_e = \int d\bm{r} d\bm{R} \Psi* \chi* \hat{H} \Psi \chi , $$

where the integration occurs over all spatial coordinates
$\set{\bm{r}_i}, \set{\bm{R}_I}$. This is a single determinant
ansatz which must lead to a mean-field description of the dynamics.
This ansatz differs from the Born-Oppenheimer approximation:

$$ \Psi_{BO} = \sum_{k=0}^{\infty} \Psi_k \chi_k , $$

even in the single-determinant limit where only
a single state $k$ is included in the expansion.
\par
Inserting this ansatz into the Schrodinger equation
reveals the following set of equations:

$$ i\hbar \frac{\partial \Psi}{\partial t} 
    = -\sum_i \frac{\hbar^2}{2m_e} \nabla_i^2 \Psi
    + \set{ \int d\bm{R} \chi^* V_{ne} \chi } \Psi , $$

$$ i\hbar \frac{\partial \chi}{\partial t} 
    = -\sum_i \frac{\hbar^2}{2m_e} \nabla_I^2 \chi
    + \set{ \int d\bm{r} \Psi^* \hat{H} \Psi } \chi . $$

This set of equations is the basis for the time-dependent
self-consistent field (TDSCF) method, wherein particles move
in time-dependent effective potentials obtained from quantum
mechanical expectation values.
\par
In the framework of classical molecular dynamics
we approximate the nuclei as classical point particles.
This can be done be rewriting the nuclear wavefunction as

$$ \chi = A \exp[iS/\hbar] , $$

with an amplitude factor $A$ and a phase $S$
which are both considered to be real.
The TDSCF equations are rewritten in terms of these variables

$$ \frac{\partial S}{\partial t} + \sum_I \frac{1}{2M_I}
    (\nabla_I S)^2 + \int d\bm{r} \Psi^* \hat{H} \Psi
    = \hbar^2 \sum_I \frac{1}{2M_I} \frac{\nabla_I^2 A}{A} , $$

$$ \frac{\partial A}{\partial t} + \sum_I \frac{1}{M_I} (\nabla_I A)
    (\nabla_I S) + \sum_I \frac{1}{2M_I} A (\nabla_I^2 S) = 0 . $$

This set of equations is known as the "quantum fluid dynamical representation".
The term for $S$ contains a term for $\hbar$ which vanishes in
the classical limit $\hbar \rightarrow 0$:

$$ \frac{\partial S}{\partial t} + \sum_I \frac{1}{2M_I}
    (\nabla_I S)^2 + \int d\bm{r} \Psi^* \hat{H} \Psi = 0 . $$

This formulation of the nuclear dynamics is isomorphic
to the Hamilton-Jacobi formulation:

$$ \frac{\partial S}{\partial t} + \hat{H} = 0 , $$

with the classical Hamilton function

$$ \hat{H} = T(P_I) + V(R_I) , $$

with coordinates $R_I$ and conjugate momenta $P_I$.
\par
If we identify the conjugate momenta

$$ \bm{P}_I = \nabla_I S , $$

we obtain the Newtonian equations of motion:

\begin{equation}
    \begin{split}
        \frac{d\bm{P}}{dt} 
    &= -\nabla_I V
    = -\nabla_I \int d\bm{r} \Psi^* \hat{H} \Psi \quad \text{or} \\
        M_I\frac{d^2 \bm{R}_I}{dt^2}
    &= -\nabla_I \int d\bm{r} \Psi^* \hat{H} \Psi \\
    &= -\nabla_I V_e^E (R_I(t)) .
    \end{split}
\end{equation}

The nuclei now move according to classical mechanics
in an effective potential $V_e^E$ generated by the electrons.
This potential is a function only of the nuclear
degrees of freedom at time $t$ after averaging out
the electronic degrees of freedom.
\par
For consistency the nuclear wavefunction appearing
in the TDSCF equation for the electronic
degrees of freedom has to be replaced by the positions
of the nuclei point particles.
This is done by replacing the nuclear density $\left| \chi \right|^2$
in the limit $\hbar \rightarrow 0$ by a product of delta functions
$ \prod_I \delta (\bm{R}_I - \bm{R}_I(t)) $ centered
at the instantaneous positions $\set{\bm{R}_I(t)}$
of the classical nuclei.
This leads to a time-dependent wave equation
for the electrons:

$$ i\hbar\frac{\partial \Psi}{\partial t} =
    -\sum_i \frac{\hbar}{2m_e} \nabla_i^2 \Psi
    + V_{ne} \Psi , $$

which evolve quantum mechanically as the nuclei propagate
classically.
This mixed approach is commonly referred to as
\textit{Ehrenfest molecular dynamics}.
Although the underlying equations describe a mean-field
theory, the Ehrenfest approach includes transitions
between electronic states.
\par
To arrive at a purely classical description of the
dynamics of both the nuclei and the electrons
we need to make further simplifications. \\
Firstly we restrict the electronic wave function $\Psi$
to the ground state wave function $\Psi_0$
at every instant of time.
This means the nuclei move on a single potential energy surface

$$ V_e^E = \int d\bm{r} \Psi_0^* \hat{H} \Psi_0 = E_0(R_I) , $$

that is determined by solving the Schrodinger equation

$$ \hat{H} \Psi_0 = E_0 \Psi_0 . $$

In this limit the Ehrenfest potential is identical
to the ground state Born-Oppenheimer potential.
\par
Since we are now only dealing with a single potential
energy surface, the problem of computing the energy surface
can be decoupled from computing the expactation values
through equation ().
First one produces an appropriate set of nuclear configurations
by solving the time-independent Schrodinger equation.
Second, these configurations are fitted to an analytical
functional form to produce a global potential energy surface.
Finally the Newtonian equations of motions are solved
on this energy surface, producing a set of classical
trajectories.
\par
To deal with the large number of degrees of freedom
as the number of nuclei in the system increases,
the global potential energy surface
is approximated as an expansion of manybody contributions:

$$ V_e^E \approx V_e^{\text{approx}} =
    \sum_I v_1(R_I) + \sum_{I < J} v_2(R_I, R_J)
    + \sum_{I < J < K} v_3(R_I, R_J, R_K) , $$

typically truncated at 2, 3 or 4-body interactions
depending on the complexity of the molecules in the system.
\par
This renders the problem of computing dynamics purely classical:

$$ M_I \frac{d^2 R_I}{dt^2} = -\nabla_I V_e^{\text{approx}} . $$

\subsection{Molecular dynamics simulations}
Classical molecular dynamics is a method
for computing equilibrium and transport properties
of manybody systems obeying classical laws of motion.
While a large number of simplifications have to be made
in order to describe quantum mechanical systems classically,
the approximation works surprisingly well
except for atoms which are quite light ($He, H^2$)
or for atoms with a vibrational energy
which is substantially larger than the thermal energy
of the system ($h\nu > k_B T$).
\par
In order to calculate properties of the system
they have to be expressed in terms of the positions
and velocities of the constituent nuclei.
For instance the temperature can be related
to the average kinetic energy of the system:

$$ \langle \frac{1}{2} m v^2 \rangle = \frac{N_f}{2} k_B T , $$

where $N_f$ is the number of degrees of freedom in our system.
At every instant of time the total kinetic energy
of our system defines an instantaneous temperature,
which has to averaged over a large number of timesteps
in order to produce the equilibrium property.
In practice, one is satisfied when the fluctuations
in the instantaneous temperature appear reasonably small.
\par
To run a molecular dynamics simulation one requires
a set of initial conditions, i.e. a set of initial positions and velocities
for every atom in the system. Typically the atoms
are placed by replicating a unit cells a number of times
in every dimension. A unit cell consists of a set
of lattice vectors which define the placement of every atom in the
unit cell. For instance the face-centered cubic cell (FCC)
contains 4 atoms:

\begin{equation}
    \begin{split}
        \bm{r}_1 &= (0, 0, 0) \\
        \bm{r}_2 &= (\frac{b}{2}, \frac{b}{2}, 0) \\
        \bm{r}_3 &= (0, \frac{b}{2}, \frac{b}{2}) \\
        \bm{r}_4 &= (\frac{b}{2}, 0, \frac{b}{2}) , \\
    \end{split}
\end{equation}

where $b$ is known as the lattice constant and defines
the size of the unit cells.
\par
The velocities are typically initialized with a random uniform
distribution or the Maxwell-Boltzmann distribution.
The Maxwell-Boltzmann distribution is the one most often used,
since the equilibrium distribution tends towards this distribution.
The exact form however will differ from the one we started with.
\par
Given these initial conditions, the system will not be in an
equilibrium state at $t=0$. To evolve the system
to an equilibrium state one most commonly evolves the system
for a number of timesteps until fluctuations in dynamic
properties such as the total potential energy or the temperature
settle down. Once we are in equilibrium we can start calculating
thermodynamic averages.
\newline
\newline
As we mentioned before, the global energy surface
is approximated as an expansion of manybody contributions:

$$ V_e^E(\set{\bm{r}}) = 
    \sum_I v_1(R_I) + \sum_{I < J} v_2(R_I, R_J)
    + \sum_{I < J < K} v_3(R_I, R_J, R_K) , $$

wherein each n-body term is an analytical function
of $n$ coordinates.
As an atom moves on the energy surface
it feels a force which is the gradient of the potential energy surface.
This means atom $i$ feels an acceleration:

$$ \bm{F}_i = m_i \frac{d^2 \bm{r}_i}{dt^2} =
    -\nabla V_e^E(\bm{r}_i, \set{\bm{r}}) . $$

For a system of $N$ atoms with only pairwise interactions
this means the forces must be calculated $N(N-1)/2$ times
for every timestep which means we have a time complexity
of order $\mathcal{O}(N^2)$. The force calculation is by far
the most important part of any molecular dynamics simulation,
and the most time consuming.
A number of techniques are employed in order to reduce
the time usage, perhaps the most common is the use of neighbor
lists. Using neighbor lists, each atom carries a list of neighbors
within a cut-off radius $r_{cut}$ and interactions
beyond this cut-off are neglected.
This reduces the time-complexity to merely 
$\mathcal{O}(cN) = \mathcal{O}(N_{r_c} \cdot N)$,
where $N_{r_c}$ is the average number of neighbors in the system
within a cutoff $r_c$. For a large system this can be a
huge reduction in complexity, but the choice of cut-off
can obviously massively impact the dynamics of the system.
\par
In order to simulate the dynamics of a system governed by
a conservative force $\bm{F} = - \nabla V_e^E$
we need to integrate the Newtonian equations of motion.
The equations of motion are typically not solvable
analytically, which means we require an effective numerical
method for integration. Some important considerations
for molecular dynamics are conservation of energy
and accuracy for large time steps.
The most common method used is the Velocity-Verlet algorithm.
At any given time step $t$, the position $\bm{r}(t + \Delta t)$
and velocity $\bm{v}(t + \Delta t)$ at the next time step
$t + \Delta t$ is calculated as:

\begin{equation}
    \begin{split}
        \bm{r}(t + \Delta t) 
        &= \bm{r}(t) + \bm{v}(t) \Delta t
        + \frac{1}{2} \bm{a}(t)\Delta t^2 , \\
        \bm{a}(t + \Delta t)
        &= -\frac{1}{m} \nabla V_e^E(\bm{r}(t + \Delta t)) , \\
        \bm{v}(t + \Delta t) 
        &= \bm{v}(t) + \frac{1}{2}
        (\bm{a}(t) + \bm{a}(t + \Delta t)) \Delta t .
    \end{split}
\end{equation}

The error in the Velocity-Verlet method is of order
$\mathcal{O}(\Delta t^2)$, which means that it is
not particularly accurate for large time steps
over a long time. However, the long term
energy drift of the method is small, which is very desirable.
It is also not very memory-intensive, which matters
for simulating very large systems.
\newline
\newline
Molecular dynamics is usually performed within
a cubic box of fixed volume $V = L_x \cdot L_y \cdot L_z$,
where $L_i$ is the length of the box in direction $i$.
Molecular dynamics is typically limited by the number
of particles we are able to simulate, of the order
$10^6 - 10^8$, which means the size of the box
is often decided by the desired density $\rho = N / V$.
Since the number of particles is always much smaller
than the number of particles in realistic systems,
approximations are required. Periodic boundary conditions
can be applied to the box in order to approximate
an infinite system. Typically particle coordinates are restricted
to the simulation box, in pseudocode:

\begin{algorithm}[H]
\caption{Continuity}
    \begin{algorithmic}
        \If{$x < -L_x / 2$}
            \State{$x \mathrel{+}= L_x$}
        \EndIf
        \If{$x > L_x / 2$}
            \State{$x \mathrel{-}= L_x$}
        \EndIf
    \end{algorithmic}
\end{algorithm}

Distance and distance vectors between particles
should also obey the minimum image convention:

\begin{algorithm}[H]
\caption{Minimum image}
    \begin{algorithmic}
        \State $dx = x_j - x_i$
        \If{$dx < -L_x / 2$}
            \State{$dx \mathrel{+}= L_x$}
        \EndIf
        \If{$dx > L_x / 2$}
            \State{$dx \mathrel{-}= L_x$}
        \EndIf
    \end{algorithmic}
\end{algorithm}

These conditions should be applied in every dimension.
This approach runs the risk of introducing nonphysical artifacts
of the simulation, such as a macromolecule interacting with its own image,
and for coulombic interactions the system must be charge neutral to avoid
summing to an infinite charge.
The optimal system size with periodic boundary conditions
will therefore depend on the intended simulation length, the desired accuracy
and the dynamics which are being studied.
\newline
\newline
Thus far we have discussed molecular dynamics for a system of $N$ particles
within a fixed volume $V$ and constant energy $E$, i.e. in the
microcanonical ensemble $NVE$. Other ensembles are also impossible,
such as the canonical ensemble $NVT$ and the isothermal-isobaric ensemble
$NPT$.
\par
Simulations in the canonical ensemble can be achieved by modifying the verlet
integration algorithm. The simplest thermostat possible
follows from the equipartion theorem:

$$ T \propto \langle mv^2 \rangle , $$

meaning some amount of kinetic energy i.e. velocity
can be added or subtracted to every atom in order to maintain
a constant temperature at every timestep.
Multiplying every velocity by a factor $\lambda = \sqrt{T_0 / T(t)}$
where $T(t)$ is the instantaneous temperature and $T_0$
is the desired temperature will achieve desired effect.
This approach significantly alters the trajectories of the system however,
which means this thermostat should only be applied for adjusting
the temperature of the system and not for taking ensemble averages
in equilibrium.
\par
The most common thermostat used is the Nosé-Hoover thermostat,
which is one of the most accurate and efficient algorithms
for achieving realistic constant-temperature conditions.
[cite: \url{https://engineering.ucsb.edu/~shell/che210d/Advanced_molecular_dynamics.pdf}]
Nosé introduced an extended Hamiltonian with two additional degrees of freedom:

\begin{itemize}
    \item $s$ - the position of an imaginary coupled heat reservoir
    \item $p_s$ - the conjugate momentum of the heat reservoir
\end{itemize}

In addition it introduces an effective mass $Q$ such that $p_s = Q\frac{ds}{dt}$.
\par
Hoover modified Nosé's approach by introducing the Hamiltonian:

$$ H = \frac{1}{2} \sum m_i \left| \bm{p}_i \right|^2 + U(\bm{r}) + \frac{\xi Q}{2}
    + 3Nk_B T \ln{s} , $$

where $\xi$ is a friction coefficient and $\bm{p}_i = m_i\bm{v}_i \times s$
are the particle momenta. This leads to a new set of Newtonian equations of motions
with an additional force that is proportional to the velocity:

\begin{equation}
\begin{split}
    \frac{d\bm{r}_i}{dt} &= \bm{v}_i \\
    \frac{d\bm{v}_i}{dt} &= - \frac{1}{m_i} \frac{\partial U(\bm{r})}{\partial \bm{r}_i}
    - \xi \bm{v}_i \\
    \frac{d\xi}{dt} &= \left( \sum m_i \left| \bm{v}_i \right|^2 - 3Nk_b T \right) / Q \\
    \frac{d \ln{s}}{dt} &= \xi .
\end{split}
\end{equation}

These can then be solved with a numerical integration scheme such as the
velocity-Verlet algorithm.

\subsection{Molecular dynamics potentials}
The dynamics of an ensemble of particles is governed by
their interactions. In molecular dynamics we stipulate that
the interactions are decided only by the relative positions
of the particles, i.e. only conservative forces act on the atoms.
This means that the force on atom $i$ is fully described
by a potential energy $U$:

$$ \bm{F}_i = -\nabla U(\bm{r}_i) , $$

which in principle depends on the position of atom $i$
and all other atoms in the system. Finding an appropriate potential
for the system of atoms which you intend to study can be ardous work,
and usually involves fitting an analytical functional form
with a large set of parameters to a potential energy surface
from ab initio quantum mechanical calculations.
\par
Potentials can be classified as either bonded or non-bonded.
Bonded potentials compute the interactions for a predefined
set of atoms and molecules in the simulations, while
non-bonded potentials compute the interactions
between \textit{all} pairs, triplets etc. of atoms
(usually within a certain radius).
Larger, more complex systems typically contain a mix of
bonded and non-bonded potentials, for example a system of rigid water molecules
interacting with a surface of carbon atoms.
\par
One of the simplest potentials meant to simulate a realistic system
is the Lennard-Jones potential:

$$ U(r_{ij}) = 4\epsilon \left(\left(\frac{\sigma}{r}\right)^{12}
    - \left(\frac{\sigma}{r}\right)^{6}\right) , $$

with $r_{ij} = \left| \bm{r}_j - \bm{r}_i \right|$.
For the Lennard-Jones potential there are only two parameters to be decided,
a characteristic energy $\epsilon$ and a characteristic length $\sigma$.
This potential is meant to emulate the relatively weak interactions
between noble gas atoms such as Argon.
It can be separated into two terms:

\begin{itemize}
    \item $\mathrel{-} \left(\frac{\sigma}{r}\right)^6$ - owing to the long-term
        attraction from van der Waals interactions
    \item $\mathrel{+} \left(\frac{\sigma}{r}\right)^{12}$ - owing to the short-term repulsion
        from the Pauli principle
\end{itemize}

While the van der Waals term is justified by theory, the repulsion term
is justified by numerical efficiency - as it contains the square of the van der Waals
term - and because it models the Pauli repulsion accurately.
\par
While the Lennard-Jones potential is very simple, requiring only two parameters
to be determined, it has shown to be effective at modelling noble gas atoms
and is commonly used as a building block for more complicated interactions.
\par
[cite \url{http://www.pages.drexel.edu/~cfa22/msim/node41.html}]
Another simple and common potential is the Stillinger-Weber potential,
which is meant to model the interactions between silicon atoms.
Silicon forms tetrahedral bonded structures as well as pairwise interactions
which means the potential includes a twobody and a threebody interaction:

\begin{equation}
    U = \sum_{i < j} v_2(r_{ij}) + \sum_{i < j < k} v_3(\bm{r}_i, \bm{r}_j, \bm{r}_k) .
\end{equation}

The twobody interaction models the pairwise interaction:

\begin{equation}
    v_2(r) =
    \begin{cases}
        \epsilon A (Br^{-p} - r^{-q}) \exp \left[ (r - a)^{-1} \right], 
        & r < a \\
        0 & r \geq a
    \end{cases}
\end{equation}

This twobody term resembles the Lennard-Jones potential,
but with an exponential cutoff.
The threebody term models the tetrahedral angles,
and is a sum over three triplets:

\begin{equation}
    v_3(\bm{r}_i, \bm{r}_j, \bm{r}_k) = h_{jik} + h_{ijk} + h_{ikj} ,
\end{equation}

with the angular interaction $h_{jik} = h(r_{ij}, r_{ik}, \theta_{jik})$ and

\begin{equation}
    h_{jik} =
    \begin{cases}
        \epsilon \lambda \exp \left[ \frac{\gamma}{r_{ij} - a}
        + \frac{\gamma}{r_{ik} - a} \right]
        \left(\cos{\theta_{jik}} - \cos{\theta_{jik}^0} \right)^2 & r_{ij} < a \\
        0 & r_{ij} \geq a
    \end{cases}
\end{equation}

where $\theta_{jik}^0$ is an "equilibrium" angle.
The terms $\epsilon, A, B, p, q, \lambda, \gamma$ are parameters
to be decided and $a$ is a cutoff radius.
